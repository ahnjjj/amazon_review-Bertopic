{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Crawling...\n",
      "End Crawling\n"
     ]
    }
   ],
   "source": [
    "import re  \n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "from urllib import request  \n",
    "from bs4 import BeautifulSoup  \n",
    " \n",
    "import io  \n",
    "import sys \n",
    " \n",
    "header = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}  \n",
    " \n",
    "print('Start Crawling...')\n",
    " \n",
    "try:  \n",
    "    data_list = []\n",
    "    \n",
    "    for r in range(26): \n",
    "        url_main=\"https://www.amazon.com/product-reviews/B0B2XSVJWX/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"\n",
    "        if r == 0:\n",
    "            url=url_main\n",
    "        else:\n",
    "            page =r +1\n",
    "            url =\"https://www.amazon.com/product-reviews/B0B2XSVJWX/ref=cm_cr_arp_d_paging_btm_next_{}?ie=UTF8&reviewerType=all_reviews&pageNumber={}\".format(page,page)\n",
    " \n",
    "        req = requests.get(url, headers=header)  \n",
    "        res = req.content\n",
    "        soup = BeautifulSoup(res,'lxml') \n",
    "        blog_items = soup.findAll('div', class_='a-section review aok-relative')\n",
    "        new_items = soup.findAll('div', class_='a-section review aok-relative')  \n",
    " \n",
    "        for item in blog_items:  \n",
    "            \n",
    "            #ID\n",
    "            ID = item.find('span','a-profile-name').text\n",
    "            '''\n",
    "            #Star Rating\n",
    "            star_rating_content = item.find('span', class_='a-icon-alt').text\n",
    "            star_rating = star_rating_content.split()[0]\n",
    "            blog=star_rating \n",
    "            '''\n",
    "            #Review_text\n",
    "            review_text=item.find('span', class_='a-size-base review-text review-text-content').text\n",
    "            \n",
    "            #Title\n",
    "            #Title = item.find('a','a-size-base a-link-normal review-title a-color-base review-title-content a-text-bold').span.text\n",
    "            \n",
    "            #Date   \n",
    "            # date_content = item.find('span', 'a-size-base a-color-secondary review-date').text\n",
    "            # date = date_content.split()[6] + date_content.split()[7] + date_content.split()[8]\n",
    "            \n",
    "            data_list.append([ID, review_text])\n",
    "        time.sleep(np.random.rand()*7)\n",
    " \n",
    "finally:\n",
    "    with open('xiaomi_miband.csv', 'w', encoding='utf-8',  newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['ID','Review_text'])\n",
    "        for u in data_list:\n",
    "            writer.writerow(u)\n",
    "            \n",
    "print('End Crawling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'July23,2022'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Crawling...\n",
      "End Crawling\n"
     ]
    }
   ],
   "source": [
    "import re  \n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "from urllib import request  \n",
    "from bs4 import BeautifulSoup  \n",
    " \n",
    "import io  \n",
    "import sys \n",
    " \n",
    "header = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}  \n",
    " \n",
    "print('Start Crawling...')\n",
    " \n",
    "try:  \n",
    "    data_list = []\n",
    "    \n",
    "    for r in range(197): \n",
    "        url_main=\"https://www.amazon.com/product-reviews/B08P57P577/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"\n",
    "        if r == 0:\n",
    "            url=url_main\n",
    "        else:\n",
    "            page =r +1\n",
    "            url =\"https://www.amazon.com/product-reviews/B08P57P577/ref=cm_cr_arp_d_paging_btm_next_{}?ie=UTF8&reviewerType=all_reviews&pageNumber={}\".format(page,page)\n",
    " \n",
    "        req = requests.get(url, headers=header)  \n",
    "        res = req.content\n",
    "        soup = BeautifulSoup(res,'lxml') \n",
    "        blog_items = soup.findAll('div', class_='a-section review aok-relative')\n",
    "        new_items = soup.findAll('div', class_='a-section review aok-relative')  \n",
    " \n",
    "        for item in blog_items:  \n",
    "            \n",
    "            #ID\n",
    "            ID = item.find('span','a-profile-name').text\n",
    "            '''\n",
    "            #Star Rating\n",
    "            star_rating_content = item.find('span', class_='a-icon-alt').text\n",
    "            star_rating = star_rating_content.split()[0]\n",
    "            blog=star_rating \n",
    "            '''\n",
    "            #Review_text\n",
    "            review_text=item.find('span', class_='a-size-base review-text review-text-content').text\n",
    "            \n",
    "            #Title\n",
    "            #Title = item.find('a','a-size-base a-link-normal review-title a-color-base review-title-content a-text-bold').span.text\n",
    "            \n",
    "            \n",
    "            \n",
    "            data_list.append([ID, review_text])\n",
    "        time.sleep(np.random.rand()*7)\n",
    " \n",
    "finally:\n",
    "    with open('xiaomi_watch_lite.csv', 'w', encoding='utf-8',  newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['ID','Review_text'])\n",
    "        for u in data_list:\n",
    "            writer.writerow(u)\n",
    "            \n",
    "print('End Crawling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Crawling...\n",
      "End Crawling\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from urllib import request\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "header = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}  \n",
    " \n",
    "print('Start Crawling...')\n",
    " \n",
    "try:  \n",
    "    data_list = []\n",
    "    \n",
    "    for r in range(62): \n",
    "        url_main=\"https://www.amazon.com/product-reviews/B08P5J7KGH/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"\n",
    "        if r == 0:\n",
    "            url=url_main\n",
    "        else:\n",
    "            page =r +1\n",
    "            url =\"https://www.amazon.com/product-reviews/B08P5J7KGH/ref=cm_cr_arp_d_paging_btm_next_{}?ie=UTF8&reviewerType=all_reviews&pageNumber={}\".format(page,page)\n",
    " \n",
    "        req = requests.get(url, headers=header)  \n",
    "        res = req.content\n",
    "        soup = BeautifulSoup(res,'lxml') \n",
    "        blog_items = soup.findAll('div', class_='a-section review aok-relative')\n",
    "        new_items = soup.findAll('div', class_='a-section review aok-relative')  \n",
    " \n",
    "        for item in blog_items:  \n",
    "            \n",
    "            #ID\n",
    "            ID = item.find('span','a-profile-name').text\n",
    "            '''\n",
    "            #Star Rating\n",
    "            star_rating_content = item.find('span', class_='a-icon-alt').text\n",
    "            star_rating = star_rating_content.split()[0]\n",
    "            blog=star_rating \n",
    "            '''\n",
    "            #Review_text\n",
    "            review_text=item.find('span', class_='a-size-base review-text review-text-content').text\n",
    "            \n",
    "            #Title\n",
    "            #Title = item.find('a','a-size-base a-link-normal review-title a-color-base review-title-content a-text-bold').span.text\n",
    "            \n",
    "            \n",
    "            data_list.append([ID, review_text])\n",
    "        time.sleep(np.random.rand()*7)\n",
    " \n",
    "finally:\n",
    "    with open('xiaomi_watch.csv', 'w', encoding='utf-8',  newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['ID','Review_text'])\n",
    "        for u in data_list:\n",
    "            writer.writerow(u)\n",
    "            \n",
    "print('End Crawling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('BERT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5d8ab4c673445a190d7bfa7fc8ab3989e375ab5b20892e401e28942cf77405d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
